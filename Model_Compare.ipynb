{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "Scni4LmIiotJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import gradio as gr\n",
        "\n",
        "# 1) íŒŒì¸íŠœë‹ëœ ë©€í‹°ë¼ë²¨ ëª¨ë¸ í´ë” ê²½ë¡œ ì„¤ì •\n",
        "#    => ë³¸ì¸ì´ ì €ì¥í•œ ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”.\n",
        "MODEL_DIR = \"/content/KcELECTRA_v2\"\n",
        "\n",
        "# 2) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "\n",
        "config = model.config\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)l\n",
        "# 3) ë¼ë²¨ ì´ë¦„ (KMHaS ë©€í‹°ë¼ë²¨ ê¸°ì¤€)\n",
        "# id2labelì´ dictë¡œ ë“¤ì–´ìˆë‹¤ê³  ê°€ì • ({\"0\": \"origin\", ...} í˜•íƒœ)\n",
        "if hasattr(config, \"id2label\") and config.id2label:\n",
        "    # keyê°€ ë¬¸ìì—´ì¼ ìˆ˜ë„ ìˆì–´ì„œ intë¡œ ì •ë ¬\n",
        "    id2label = {int(k): v for k, v in config.id2label.items()}\n",
        "    label_names = [id2label[i] for i in range(len(id2label))]\n",
        "else:\n",
        "    # í˜¹ì‹œ id2labelì´ ì—†ìœ¼ë©´, ê·¸ë•Œë§Œ ìˆ˜ë™ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©\n",
        "    label_names = [\n",
        "        \"origin\",\n",
        "        \"physical\",\n",
        "        \"politics\",\n",
        "        \"profanity\",\n",
        "        \"age\",\n",
        "        \"gender\",\n",
        "        \"race\",\n",
        "        \"religion\",\n",
        "    ]\n",
        "\n",
        "\n",
        "# 4) í•œ ë¬¸ì¥ì— ëŒ€í•´ ë©€í‹°ë¼ë²¨ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
        "def predict_hatespeech(text, threshold=0.5):\n",
        "    if not text.strip():\n",
        "        return pd.DataFrame(columns=[\"label\", \"prob\", \"pred\"])\n",
        "\n",
        "    # í† í¬ë‚˜ì´ì§•\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # shape: [1, num_labels]\n",
        "\n",
        "    # ì‹œê·¸ëª¨ì´ë“œë¡œ í™•ë¥ ë¡œ ë³€í™˜\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()[0]  # [num_labels]\n",
        "    preds = (probs >= float(threshold)).astype(int)\n",
        "\n",
        "    # íŒë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬\n",
        "    df = pd.DataFrame({\n",
        "        \"label\": label_names,\n",
        "        \"prob\": np.round(probs, 3),\n",
        "        \"pred(0/1)\": preds,\n",
        "    })\n",
        "\n",
        "    # ë³´ê¸° ì¢‹ê²Œ ë¼ë²¨ ìˆœì„œ ê³ ì •\n",
        "    return df\n",
        "\n",
        "# 5) Gradio UI ì •ì˜\n",
        "description_md = \"\"\"\n",
        "### í•œêµ­ì–´ Hate Speech ë©€í‹°ë¼ë²¨ ì˜ˆì¸¡ ë°ëª¨\n",
        "\n",
        "- ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ 8ê°œ ë¼ë²¨ì— ëŒ€í•´ í™•ë¥ ê³¼ 0/1 ì˜ˆì¸¡ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "- `threshold`ë¥¼ ì¡°ì •í•´ì„œ, ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ ì¡ì„ì§€ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_hatespeech,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            lines=3,\n",
        "            label=\"ì…ë ¥ ë¬¸ì¥\",\n",
        "            placeholder=\"ê²€ì‚¬í•´ë³´ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì ì–´ë³´ì„¸ìš”.\",\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=0.9,\n",
        "            value=0.5,\n",
        "            step=0.05,\n",
        "            label=\"threshold (ê¸°ë³¸ 0.5)\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs=gr.Dataframe(\n",
        "        headers=[\"label\", \"prob\", \"pred(0/1)\"],\n",
        "        label=\"ì˜ˆì¸¡ ê²°ê³¼\",\n",
        "    ),\n",
        "    title=\"KMHaS ë©€í‹°ë¼ë²¨ Hate Speech íƒì§€ ë°ëª¨\",\n",
        "    description=description_md,\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "hGLZYOyfh05Y",
        "outputId": "65dc0f1d-2dfa-4aa7-827d-16c9aa995220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://daac3fe0cafb6e75a9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://daac3fe0cafb6e75a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "7S0aypZu70jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# =====================================\n",
        "# 1. ê³µí†µ ì„¤ì •\n",
        "# =====================================\n",
        "\n",
        "# KMHaS ë©€í‹°ë¼ë²¨ ê¸°ì¤€ ë¼ë²¨ ì´ë¦„\n",
        "DEFAULT_LABELS = [\n",
        "    \"origin\",    # ì¶œì‹ /ì§€ì—­ í˜ì˜¤\n",
        "    \"physical\",  # ì™¸ëª¨/ì‹ ì²´ ë¹„í•˜\n",
        "    \"politics\",  # ì •ì¹˜/ì´ë…\n",
        "    \"profanity\", # ì¼ë°˜ ìš•ì„¤\n",
        "    \"age\",       # ì—°ë ¹\n",
        "    \"gender\",    # ì„±ë³„\n",
        "    \"race\",      # ì¸ì¢…/ì´ì£¼ë¯¼\n",
        "    \"religion\",  # ì¢…êµ\n",
        "]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ğŸ‘‰ ì—¬ê¸°ë§Œ ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
        "#    (Trainerì—ì„œ save_pretrained í–ˆë˜ ë””ë ‰í† ë¦¬)\n",
        "MODEL_CONFIGS = {\n",
        "    \"KcELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KcELECTRA_v2\",  # ì˜ˆì‹œ: ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •\n",
        "        \"tokenizer_name\": \"beomi/KcELECTRA-base-v2022\",\n",
        "    },\n",
        "    \"KoELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KoELECTRA_v2\",  # ì˜ˆì‹œ\n",
        "        \"tokenizer_name\": \"monologg/koelectra-base-v3-discriminator\",\n",
        "    },\n",
        "    \"KR-Medium v1\": {\n",
        "        \"model_dir\": \"/content/krmedium_v1\",   # ì˜ˆì‹œ\n",
        "        \"tokenizer_name\": \"snunlp/KR-Medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# ëª¨ë¸/í† í¬ë‚˜ì´ì € ìºì‹œ\n",
        "_model_cache = {}\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer(model_key: str):\n",
        "    \"\"\"\n",
        "    Gradio ì‹¤í–‰ ì‹œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œë˜ê¸° ë•Œë¬¸ì—\n",
        "    í•œ ë²ˆ ë¡œë“œí•œ ëª¨ë¸ì€ ìºì‹œì— ë³´ê´€í•´ì„œ ì¬ì‚¬ìš©.\n",
        "    \"\"\"\n",
        "    if model_key in _model_cache:\n",
        "        return _model_cache[model_key]\n",
        "\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    model_dir = cfg[\"model_dir\"]\n",
        "    tokenizer_name = cfg[\"tokenizer_name\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # id2labelì´ configì— ë“¤ì–´ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ DEFAULT_LABELS ì‚¬ìš©\n",
        "    id2label = getattr(model.config, \"id2label\", None)\n",
        "    num_labels = model.config.num_labels\n",
        "\n",
        "    if id2label and len(id2label) == num_labels:\n",
        "        labels = []\n",
        "        for i in range(num_labels):\n",
        "            if i in id2label:\n",
        "                labels.append(id2label[i])\n",
        "            elif str(i) in id2label:\n",
        "                labels.append(id2label[str(i)])\n",
        "            else:\n",
        "                labels.append(f\"label_{i}\")\n",
        "    else:\n",
        "        labels = DEFAULT_LABELS[:num_labels]\n",
        "\n",
        "    _model_cache[model_key] = (tokenizer, model, labels)\n",
        "    return tokenizer, model, labels\n",
        "\n",
        "\n",
        "def run_one_model(model_key: str, text: str, threshold: float, max_len: int = 128):\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´:\n",
        "    - text â†’ í† í¬ë‚˜ì´ì¦ˆ â†’ ëª¨ë¸ forward\n",
        "    - sigmoid(prob), thresholdë¡œ pred ê³„ì‚°\n",
        "    - DataFrame ë°˜í™˜\n",
        "    \"\"\"\n",
        "    tokenizer, model, labels = load_model_and_tokenizer(model_key)\n",
        "\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        logits = outputs.logits[0]  # (num_labels,)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"label\": labels,\n",
        "            \"prob\": probs,\n",
        "            \"pred\": preds,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # ë³´ê¸° ì¢‹ê²Œ prob ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
        "    df = df.sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def compare_models(text: str, threshold: float):\n",
        "    \"\"\"\n",
        "    Gradioì—ì„œ í˜¸ì¶œí•  í•¨ìˆ˜.\n",
        "    ì…ë ¥ ë¬¸ì¥ê³¼ thresholdë¥¼ ë°›ì•„\n",
        "    3ê°œ ëª¨ë¸ì˜ DataFrameì„ ìˆœì„œëŒ€ë¡œ ë°˜í™˜.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        empty_df = pd.DataFrame({\"label\": [], \"prob\": [], \"pred\": []})\n",
        "        return empty_df, empty_df, empty_df\n",
        "\n",
        "    df_kc = run_one_model(\"KcELECTRA v2\", text, threshold)\n",
        "    df_ko = run_one_model(\"KoELECTRA v2\", text, threshold)\n",
        "    df_kr = run_one_model(\"KR-Medium v1\", text, threshold)\n",
        "\n",
        "    return df_kc, df_ko, df_kr\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 2. Gradio UI ì •ì˜\n",
        "# =====================================\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # í•œêµ­ì–´ Hate Speech ë©€í‹°ë¼ë²¨ â€“ 3ê°œ ë°±ë³¸ ë¹„êµ ë·°ì–´ (Gradio)\n",
        "\n",
        "        í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´,\n",
        "        **KcELECTRA / KoELECTRA / KR-Medium** ì„¸ ëª¨ë¸ì´\n",
        "        ê° ë¼ë²¨ì— ëŒ€í•´ ì–´ë–¤ í™•ë¥ ê³¼ ì˜ˆì¸¡ê°’(0/1)ì„ ë‚´ëŠ”ì§€ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(\n",
        "            label=\"ì…ë ¥ ë¬¸ì¥\",\n",
        "            value=\"ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.\",\n",
        "            lines=3,\n",
        "        )\n",
        "    with gr.Row():\n",
        "        threshold_input = gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=0.9,\n",
        "            step=0.05,\n",
        "            value=0.5,\n",
        "            label=\"Threshold (prob â‰¥ threshold â†’ pred=1)\",\n",
        "        )\n",
        "\n",
        "    run_btn = gr.Button(\"3ê°œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°\")\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KcELECTRA v2 ê²°ê³¼\")\n",
        "            out_kc = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KoELECTRA v2 ê²°ê³¼\")\n",
        "            out_ko = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KR-Medium v1 ê²°ê³¼\")\n",
        "            out_kr = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=compare_models,\n",
        "        inputs=[text_input, threshold_input],\n",
        "        outputs=[out_kc, out_ko, out_kr],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "nn7vdtl07siM",
        "outputId": "6493111b-c02e-4ece-b418-d54c3a51a0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6a49a764284c31ec29.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6a49a764284c31ec29.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for k, cfg in MODEL_CONFIGS.items():\n",
        "    print(\"===\", k, \"===\")\n",
        "    print(\"model_dir:\", cfg[\"model_dir\"])\n",
        "    print(\"exists?:\", os.path.isdir(cfg[\"model_dir\"]))\n",
        "    if os.path.isdir(cfg[\"model_dir\"]):\n",
        "        print(\"files:\", os.listdir(cfg[\"model_dir\"]))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdatMwr7_maO",
        "outputId": "2cbaee80-df97-4c77-e520-6737ef667564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== KcELECTRA v2 ===\n",
            "model_dir: /content/KcELECTRA_v2\n",
            "exists?: True\n",
            "files: ['special_tokens_map.json', 'tokenizer.json', 'vocab.txt', 'model.safetensors', 'config.json', 'tokenizer_config.json', '.ipynb_checkpoints']\n",
            "\n",
            "=== KoELECTRA v2 ===\n",
            "model_dir: /content/KoELECTRA_v2\n",
            "exists?: True\n",
            "files: ['special_tokens_map.json', 'tokenizer.json', 'vocab.txt', 'model.safetensors', 'config.json', 'tokenizer_config.json']\n",
            "\n",
            "=== KR-Medium v1 ===\n",
            "model_dir: /content/krmedium_v1\n",
            "exists?: True\n",
            "files: ['special_tokens_map.json', 'tokenizer.json', 'vocab.txt', 'model.safetensors', 'config.json', 'tokenizer_config.json', '.ipynb_checkpoints']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_DIR = \"/content/Krmedium_v1\"  # ì‹¤ì œ ê²½ë¡œ\n",
        "BASE_NAME = \"snunlp/KR-Medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
        "model.eval()\n",
        "\n",
        "text = \"ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.\"\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded)\n",
        "    logits = outputs.logits[0]\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "print(\"logits:\", logits)\n",
        "print(\"probs:\", probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "4ieROJKFADL2",
        "outputId": "386cd959-6ef6-4490-b5f5-2b87303f2e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/Krmedium_v1'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/Krmedium_v1'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-666261971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    509\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/Krmedium_v1'. Use `repo_type` argument if needed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "QoWq-c6fA2qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEFAULT_LABELS = [\"origin\",\"physical\",\"politics\",\"profanity\",\"age\",\"gender\",\"race\",\"religion\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    \"KcELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KcELECTRA_v2\",  # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •\n",
        "        \"tokenizer_name\": \"beomi/KcELECTRA-base-v2022\",\n",
        "    },\n",
        "    \"KoELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KoELECTRA_v2\",\n",
        "        \"tokenizer_name\": \"monologg/koelectra-base-v3-discriminator\",\n",
        "    },\n",
        "    \"KR-Medium v1\": {\n",
        "        \"model_dir\": \"/content/krmedium_v1\",\n",
        "        \"tokenizer_name\": \"snunlp/KR-Medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "_model_cache = {}\n",
        "_error_log = gr.State(\"\")\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer(model_key: str):\n",
        "    if model_key in _model_cache:\n",
        "        return _model_cache[model_key]\n",
        "\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(cfg[\"model_dir\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    id2label = getattr(model.config, \"id2label\", None)\n",
        "    num_labels = model.config.num_labels\n",
        "\n",
        "    if id2label and len(id2label) == num_labels:\n",
        "        labels = []\n",
        "        for i in range(num_labels):\n",
        "            if i in id2label:\n",
        "                labels.append(id2label[i])\n",
        "            elif str(i) in id2label:\n",
        "                labels.append(id2label[str(i)])\n",
        "            else:\n",
        "                labels.append(f\"label_{i}\")\n",
        "    else:\n",
        "        labels = DEFAULT_LABELS[:num_labels]\n",
        "\n",
        "    _model_cache[model_key] = (tokenizer, model, labels)\n",
        "    return tokenizer, model, labels\n",
        "\n",
        "\n",
        "def run_one_model_safe(model_key: str, text: str, threshold: float, max_len: int = 128):\n",
        "    try:\n",
        "        tokenizer, model, labels = load_model_and_tokenizer(model_key)\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_len,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "            logits = outputs.logits[0]\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        df = pd.DataFrame({\"label\": labels, \"prob\": probs, \"pred\": preds})\n",
        "        df = df.sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
        "        return df, \"\"\n",
        "    except Exception as e:\n",
        "        # ì—ëŸ¬ ë°œìƒ ì‹œ, ë¹ˆ ë°ì´í„°í”„ë ˆì„ê³¼ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜\n",
        "        empty_df = pd.DataFrame({\"label\": [], \"prob\": [], \"pred\": []})\n",
        "        return empty_df, f\"[{model_key}] ì—ëŸ¬: {repr(e)}\"\n",
        "\n",
        "\n",
        "def compare_models(text: str, threshold: float):\n",
        "    if not text.strip():\n",
        "        empty_df = pd.DataFrame({\"label\": [], \"prob\": [], \"pred\": []})\n",
        "        return empty_df, empty_df, empty_df, \"ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    df_kc, err_kc = run_one_model_safe(\"KcELECTRA v2\", text, threshold)\n",
        "    df_ko, err_ko = run_one_model_safe(\"KoELECTRA v2\", text, threshold)\n",
        "    df_kr, err_kr = run_one_model_safe(\"KR-Medium v1\", text, threshold)\n",
        "\n",
        "    all_err = \"\\n\".join([m for m in [err_kc, err_ko, err_kr] if m])\n",
        "    if not all_err:\n",
        "        all_err = \"ì—ëŸ¬ ì—†ìŒ.\"\n",
        "\n",
        "    return df_kc, df_ko, df_kr, all_err\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# í•œêµ­ì–´ Hate Speech â€“ 3ê°œ ëª¨ë¸ ë¹„êµ (Gradio)\")\n",
        "\n",
        "    text_input = gr.Textbox(\n",
        "        label=\"ì…ë ¥ ë¬¸ì¥\",\n",
        "        value=\"ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.\",\n",
        "        lines=3,\n",
        "    )\n",
        "    threshold_input = gr.Slider(\n",
        "        minimum=0.1, maximum=0.9, step=0.05, value=0.5,\n",
        "        label=\"Threshold (prob â‰¥ threshold â†’ pred=1)\",\n",
        "    )\n",
        "    run_btn = gr.Button(\"3ê°œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°\")\n",
        "\n",
        "    with gr.Row():\n",
        "        out_kc = gr.Dataframe(label=\"KcELECTRA v2\")\n",
        "        out_ko = gr.Dataframe(label=\"KoELECTRA v2\")\n",
        "        out_kr = gr.Dataframe(label=\"KR-Medium v1\")\n",
        "\n",
        "    err_box = gr.Textbox(label=\"ì—ëŸ¬ ë¡œê·¸\", lines=4)\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=compare_models,\n",
        "        inputs=[text_input, threshold_input],\n",
        "        outputs=[out_kc, out_ko, out_kr, err_box],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "KB7Au4QKAz52",
        "outputId": "86f5e577-d108-4dcd-c724-a7c6e2158eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://29a05e8bde63e20176.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://29a05e8bde63e20176.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "vM5Wxz-fDsns"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 1. ê³µí†µ ì„¤ì •\n",
        "# ==============================\n",
        "\n",
        "DEFAULT_LABELS = [\n",
        "    \"origin\",    # ì¶œì‹ /ì§€ì—­\n",
        "    \"physical\",  # ì™¸ëª¨/ì‹ ì²´\n",
        "    \"politics\",  # ì •ì¹˜/ì´ë…\n",
        "    \"profanity\", # ì¼ë°˜ ìš•ì„¤\n",
        "    \"age\",       # ì—°ë ¹\n",
        "    \"gender\",    # ì„±ë³„\n",
        "    \"race\",      # ì¸ì¢…/ì´ì£¼ë¯¼\n",
        "    \"religion\",  # ì¢…êµ\n",
        "]\n",
        "\n",
        "# ë¼ë²¨ â†’ í•œêµ­ì–´ ì„¤ëª… ë§¤í•‘ (ìš”ì•½ ë¬¸ì¥ìš©)\n",
        "LABEL_KO = {\n",
        "    \"origin\":    \"origin (ì¶œì‹ /ì§€ì—­)\",\n",
        "    \"physical\":  \"physical (ì™¸ëª¨/ì‹ ì²´)\",\n",
        "    \"politics\":  \"politics (ì •ì¹˜/ì´ë…)\",\n",
        "    \"profanity\": \"profanity (ì¼ë°˜ ìš•ì„¤)\",\n",
        "    \"age\":       \"age (ì—°ë ¹)\",\n",
        "    \"gender\":    \"gender (ì„±ë³„)\",\n",
        "    \"race\":      \"race (ì¸ì¢…/ì´ì£¼ë¯¼)\",\n",
        "    \"religion\":  \"religion (ì¢…êµ)\",\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ğŸ‘‰ ê²½ë¡œë¥¼ ë‹¹ì‹ ì˜ ëª¨ë¸ ì €ì¥ í´ë”ë¡œ ë§ì¶°ì£¼ì„¸ìš”.\n",
        "MODEL_CONFIGS = {\n",
        "    \"KcELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KcELECTRA_v2\",\n",
        "        \"tokenizer_name\": \"beomi/KcELECTRA-base-v2022\",\n",
        "    },\n",
        "    \"KoELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KoELECTRA_v2\",\n",
        "        \"tokenizer_name\": \"monologg/koelectra-base-v3-discriminator\",\n",
        "    },\n",
        "    \"KR-Medium v1\": {\n",
        "        \"model_dir\": \"/content/krmedium_v1\",\n",
        "        \"tokenizer_name\": \"snunlp/KR-Medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "_model_cache = {}\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. ëª¨ë¸ ë¡œë“œ\n",
        "# ==============================\n",
        "\n",
        "def load_model_and_tokenizer(model_key: str):\n",
        "    if model_key in _model_cache:\n",
        "        return _model_cache[model_key]\n",
        "\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(cfg[\"model_dir\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    id2label = getattr(model.config, \"id2label\", None)\n",
        "    num_labels = model.config.num_labels\n",
        "\n",
        "    if id2label and len(id2label) == num_labels:\n",
        "        labels = []\n",
        "        for i in range(num_labels):\n",
        "            if i in id2label:\n",
        "                labels.append(id2label[i])\n",
        "            elif str(i) in id2label:\n",
        "                labels.append(id2label[str(i)])\n",
        "            else:\n",
        "                labels.append(f\"label_{i}\")\n",
        "    else:\n",
        "        labels = DEFAULT_LABELS[:num_labels]\n",
        "\n",
        "    _model_cache[model_key] = (tokenizer, model, labels)\n",
        "    return tokenizer, model, labels\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰ + ìš”ì•½\n",
        "# ==============================\n",
        "\n",
        "def summarize_labels(df: pd.DataFrame, threshold: float) -> str:\n",
        "    \"\"\"\n",
        "    DataFrame(label, prob, pred)ì„ ë°›ì•„ì„œ\n",
        "    ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ í•œ ì¤„/ë‘ ì¤„ ìš”ì•½ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì¤Œ.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return \"ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ (ëª¨ë¸ ì¶œë ¥ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤).\"\n",
        "\n",
        "    positive = df[df[\"pred\"] == 1]\n",
        "\n",
        "    if len(positive) == 0:\n",
        "        return f\"ì˜ˆì¸¡ ë¼ë²¨: í˜ì˜¤ ì—†ìŒ (ëª¨ë“  ë¼ë²¨ prob < {threshold:.2f})\"\n",
        "\n",
        "    items = []\n",
        "    for _, row in positive.iterrows():\n",
        "        label = row[\"label\"]\n",
        "        prob = float(row[\"prob\"])\n",
        "        ko = LABEL_KO.get(label, label)\n",
        "        items.append(f\"{ko} (prob={prob:.3f})\")\n",
        "\n",
        "    joined = \", \".join(items)\n",
        "    return f\"ì˜ˆì¸¡ ë¼ë²¨: {joined}\"\n",
        "\n",
        "\n",
        "def run_one_model_safe(model_key: str, text: str, threshold: float, max_len: int = 128):\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´:\n",
        "    - ì—ëŸ¬ ì—†ì´ DataFrame + ìš”ì•½ë¬¸ + ì—ëŸ¬ë©”ì‹œì§€ë¥¼ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokenizer, model, labels = load_model_and_tokenizer(model_key)\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_len,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "            logits = outputs.logits[0]\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "\n",
        "        df = pd.DataFrame({\"label\": labels, \"prob\": probs, \"pred\": preds})\n",
        "        df = df.sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "        summary = summarize_labels(df, threshold)\n",
        "        return df, summary, \"\"\n",
        "    except Exception as e:\n",
        "        empty_df = pd.DataFrame({\"label\": [], \"prob\": [], \"pred\": []})\n",
        "        summary = f\"ì˜ˆì¸¡ ì‹¤íŒ¨: {repr(e)}\"\n",
        "        return empty_df, summary, f\"[{model_key}] ì—ëŸ¬: {repr(e)}\"\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. Gradioì—ì„œ í˜¸ì¶œí•  ë©”ì¸ í•¨ìˆ˜\n",
        "# ==============================\n",
        "\n",
        "def compare_models(text: str, threshold: float):\n",
        "    if not text.strip():\n",
        "        empty_df = pd.DataFrame({\"label\": [], \"prob\": [], \"pred\": []})\n",
        "        msg = \"ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "        return (\n",
        "            empty_df, empty_df, empty_df,  # DataFrames\n",
        "            \"ì˜ˆì¸¡ ì—†ìŒ\", \"ì˜ˆì¸¡ ì—†ìŒ\", \"ì˜ˆì¸¡ ì—†ìŒ\",  # summaries\n",
        "            msg,\n",
        "        )\n",
        "\n",
        "    df_kc, sum_kc, err_kc = run_one_model_safe(\"KcELECTRA v2\", text, threshold)\n",
        "    df_ko, sum_ko, err_ko = run_one_model_safe(\"KoELECTRA v2\", text, threshold)\n",
        "    df_kr, sum_kr, err_kr = run_one_model_safe(\"KR-Medium v1\", text, threshold)\n",
        "\n",
        "    all_err = \"\\n\".join([m for m in [err_kc, err_ko, err_kr] if m])\n",
        "    if not all_err:\n",
        "        all_err = \"ì—ëŸ¬ ì—†ìŒ.\"\n",
        "\n",
        "    return df_kc, df_ko, df_kr, sum_kc, sum_ko, sum_kr, all_err\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. Gradio UI\n",
        "# ==============================\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # í•œêµ­ì–´ Hate Speech ë©€í‹°ë¼ë²¨ â€“ 3ê°œ ëª¨ë¸ ë¹„êµ (Gradio)\n",
        "\n",
        "        í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´,\n",
        "        **KcELECTRA / KoELECTRA / KR-Medium** ì„¸ ëª¨ë¸ì´\n",
        "        ê° ë¼ë²¨ì— ëŒ€í•´ ì–´ë–¤ í™•ë¥ ê³¼ ì˜ˆì¸¡ê°’(0/1)ì„ ë‚´ëŠ”ì§€ ë¹„êµí•˜ê³ ,\n",
        "        ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ ë¼ë²¨ì„ \"í˜ì˜¤(1)\"ë¡œ íŒë‹¨í–ˆëŠ”ì§€ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    text_input = gr.Textbox(\n",
        "        label=\"ì…ë ¥ ë¬¸ì¥\",\n",
        "        value=\"ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.\",\n",
        "        lines=3,\n",
        "    )\n",
        "    threshold_input = gr.Slider(\n",
        "        minimum=0.1,\n",
        "        maximum=0.9,\n",
        "        step=0.05,\n",
        "        value=0.5,\n",
        "        label=\"Threshold (prob â‰¥ threshold â†’ pred=1)\",\n",
        "    )\n",
        "    run_btn = gr.Button(\"3ê°œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KcELECTRA v2 ê²°ê³¼\")\n",
        "            out_kc = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "            summary_kc = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KoELECTRA v2 ê²°ê³¼\")\n",
        "            out_ko = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "            summary_ko = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KR-Medium v1 ê²°ê³¼\")\n",
        "            out_kr = gr.Dataframe(\n",
        "                headers=[\"label\", \"prob\", \"pred\"],\n",
        "                datatype=[\"str\", \"number\", \"number\"],\n",
        "                interactive=False,\n",
        "            )\n",
        "            summary_kr = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "\n",
        "    err_box = gr.Textbox(label=\"ì—ëŸ¬ ë¡œê·¸\", lines=4)\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=compare_models,\n",
        "        inputs=[text_input, threshold_input],\n",
        "        outputs=[out_kc, out_ko, out_kr, summary_kc, summary_ko, summary_kr, err_box],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "8hzKSJniDpmD",
        "outputId": "bec08cc9-a393-4fcd-9da0-155f8d3d0f13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cd0d5bfd5a15ec118d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cd0d5bfd5a15ec118d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1. ê³µí†µ ì„¤ì •\n",
        "# -----------------------------\n",
        "DEFAULT_LABELS = [\n",
        "    \"origin\",    # ì¶œì‹ /ì§€ì—­\n",
        "    \"physical\",  # ì™¸ëª¨/ì‹ ì²´\n",
        "    \"politics\",  # ì •ì¹˜/ì´ë…\n",
        "    \"profanity\", # ì¼ë°˜ ìš•ì„¤\n",
        "    \"age\",       # ì—°ë ¹\n",
        "    \"gender\",    # ì„±ë³„\n",
        "    \"race\",      # ì¸ì¢…/ì´ì£¼ë¯¼\n",
        "    \"religion\",  # ì¢…êµ\n",
        "]\n",
        "\n",
        "LABEL_KO = {\n",
        "    \"origin\":    \"origin (ì¶œì‹ /ì§€ì—­)\",\n",
        "    \"physical\":  \"physical (ì™¸ëª¨/ì‹ ì²´)\",\n",
        "    \"politics\":  \"politics (ì •ì¹˜/ì´ë…)\",\n",
        "    \"profanity\": \"profanity (ì¼ë°˜ ìš•ì„¤)\",\n",
        "    \"age\":       \"age (ì—°ë ¹)\",\n",
        "    \"gender\":    \"gender (ì„±ë³„)\",\n",
        "    \"race\":      \"race (ì¸ì¢…/ì´ì£¼ë¯¼)\",\n",
        "    \"religion\":  \"religion (ì¢…êµ)\",\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ğŸ‘‰ ì—¬ê¸° ê²½ë¡œë¥¼ ë³¸ì¸ Colab í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
        "MODEL_CONFIGS = {\n",
        "    \"KcELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KcELECTRA_v2\",\n",
        "        \"tokenizer_name\": \"beomi/KcELECTRA-base-v2022\",\n",
        "    },\n",
        "    \"KoELECTRA v2\": {\n",
        "        \"model_dir\": \"/content/KoELECTRA_v2\",\n",
        "        \"tokenizer_name\": \"monologg/koelectra-base-v3-discriminator\",\n",
        "    },\n",
        "    \"KR-Medium v1\": {\n",
        "        \"model_dir\": \"/content/krmedium_v1\",\n",
        "        \"tokenizer_name\": \"snunlp/KR-Medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "_model_cache = {}\n",
        "\n",
        "# -----------------------------\n",
        "# 2. ëª¨ë¸ ë¡œë“œ\n",
        "# -----------------------------\n",
        "def load_model_and_tokenizer(model_key: str):\n",
        "    if model_key in _model_cache:\n",
        "        return _model_cache[model_key]\n",
        "\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(cfg[\"model_dir\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # id2labelì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì“°ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ ë¼ë²¨ ì‚¬ìš©\n",
        "    id2label = getattr(model.config, \"id2label\", None)\n",
        "    num_labels = model.config.num_labels\n",
        "\n",
        "    if id2label and len(id2label) == num_labels:\n",
        "        labels = []\n",
        "        for i in range(num_labels):\n",
        "            if i in id2label:\n",
        "                labels.append(id2label[i])\n",
        "            elif str(i) in id2label:\n",
        "                labels.append(id2label[str(i)])\n",
        "            else:\n",
        "                labels.append(f\"label_{i}\")\n",
        "    else:\n",
        "        labels = DEFAULT_LABELS[:num_labels]\n",
        "\n",
        "    _model_cache[model_key] = (tokenizer, model, labels)\n",
        "    return tokenizer, model, labels\n",
        "\n",
        "# -----------------------------\n",
        "# 3. ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ + ìš”ì•½ë¬¸ ìƒì„±\n",
        "# -----------------------------\n",
        "def summarize_positive_labels(labels, probs, threshold: float) -> str:\n",
        "    \"\"\"\n",
        "    ë¼ë²¨ ëª©ë¡ + í™•ë¥  ë°°ì—´ + threshold ë¥¼ ë°›ì•„ì„œ\n",
        "    ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ ìš”ì•½ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜.\n",
        "    \"\"\"\n",
        "    positive = [\n",
        "        (lab, float(p))\n",
        "        for lab, p in zip(labels, probs)\n",
        "        if p >= threshold\n",
        "    ]\n",
        "\n",
        "    if not positive:\n",
        "        return f\"ì˜ˆì¸¡ ë¼ë²¨: í˜ì˜¤ ì—†ìŒ (ëª¨ë“  ë¼ë²¨ prob < {threshold:.2f})\"\n",
        "\n",
        "    parts = []\n",
        "    for lab, p in positive:\n",
        "        ko = LABEL_KO.get(lab, lab)\n",
        "        parts.append(f\"{ko} (prob={p:.3f})\")\n",
        "\n",
        "    return \"ì˜ˆì¸¡ ë¼ë²¨: \" + \", \".join(parts)\n",
        "\n",
        "\n",
        "def run_one_model_safe(model_key: str, text: str, threshold: float, max_len: int = 128):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ 1ê°œì— ëŒ€í•´:\n",
        "    - ìš”ì•½ ë¬¸ìì—´, ì—ëŸ¬ë©”ì‹œì§€ë¥¼ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokenizer, model, labels = load_model_and_tokenizer(model_key)\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_len,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "            logits = outputs.logits[0]\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        summary = summarize_positive_labels(labels, probs, threshold)\n",
        "        return summary, \"\"\n",
        "    except Exception as e:\n",
        "        return f\"ì˜ˆì¸¡ ì‹¤íŒ¨: {repr(e)}\", f\"[{model_key}] ì—ëŸ¬: {repr(e)}\"\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Gradioì—ì„œ ì“¸ ë©”ì¸ í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def compare_models(text: str, threshold: float):\n",
        "    if not text.strip():\n",
        "        msg = \"ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "        return \"ì˜ˆì¸¡ ì—†ìŒ\", \"ì˜ˆì¸¡ ì—†ìŒ\", \"ì˜ˆì¸¡ ì—†ìŒ\", msg\n",
        "\n",
        "    sum_kc, err_kc = run_one_model_safe(\"KcELECTRA v2\", text, threshold)\n",
        "    sum_ko, err_ko = run_one_model_safe(\"KoELECTRA v2\", text, threshold)\n",
        "    sum_kr, err_kr = run_one_model_safe(\"KR-Medium v1\", text, threshold)\n",
        "\n",
        "    all_err = \"\\n\".join([m for m in [err_kc, err_ko, err_kr] if m])\n",
        "    if not all_err:\n",
        "        all_err = \"ì—ëŸ¬ ì—†ìŒ.\"\n",
        "\n",
        "    return sum_kc, sum_ko, sum_kr, all_err\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Gradio UI\n",
        "# -----------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # í•œêµ­ì–´ Hate Speech ë©€í‹°ë¼ë²¨ â€“ 3ê°œ ëª¨ë¸ 'ìµœì¢… ë¼ë²¨' ë¹„êµ\n",
        "\n",
        "        í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´\n",
        "        **KcELECTRA / KoELECTRA / KR-Medium** ê° ëª¨ë¸ì´\n",
        "        ì–´ë–¤ í˜ì˜¤ ë¼ë²¨ì„ 1ë¡œ íŒë‹¨í–ˆëŠ”ì§€ë§Œ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    text_input = gr.Textbox(\n",
        "        label=\"ì…ë ¥ ë¬¸ì¥\",\n",
        "        value=\"ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.\",\n",
        "        lines=3,\n",
        "    )\n",
        "    threshold_input = gr.Slider(\n",
        "        minimum=0.1,\n",
        "        maximum=0.9,\n",
        "        step=0.05,\n",
        "        value=0.5,\n",
        "        label=\"Threshold (prob â‰¥ threshold â†’ pred=1)\",\n",
        "    )\n",
        "    run_btn = gr.Button(\"3ê°œ ëª¨ë¸ë¡œ ë¶„ì„í•˜ê¸°\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KcELECTRA v2\")\n",
        "            summary_kc = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KoELECTRA v2\")\n",
        "            summary_ko = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### KR-Medium v1\")\n",
        "            summary_kr = gr.Markdown(\"ì˜ˆì¸¡ ë¼ë²¨: -\")\n",
        "\n",
        "    err_box = gr.Textbox(label=\"ì—ëŸ¬ ë¡œê·¸\", lines=4)\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=compare_models,\n",
        "        inputs=[text_input, threshold_input],\n",
        "        outputs=[summary_kc, summary_ko, summary_kr, err_box],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "XtK1PWlnFzWu",
        "outputId": "412df4ee-fe42-46dd-cd1e-9638095038ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://46ed915234c58682f1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://46ed915234c58682f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}