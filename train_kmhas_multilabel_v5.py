import os
import json
import time
import random
import math
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    get_scheduler,
)

# ============================================================
# 0) ì„¤ì • (v5 = v3 ê¸°ë°˜ + LR scheduler ì ìš©)
# ============================================================
MODEL_NAME = "beomi/KcELECTRA-base-v2022"

RUN_NAME = "v5_lr_3e-5_cosine_warmup"
CHANGE_NOTE = "v4(train_bs=64) ë„ˆë¬´ ëŠë ¤ì„œ v3(train_bs=32)ë¡œ ë³µê·€ + cosine lr scheduler(warmup) ì ìš©"

SAVE_DIR = "./project/kmhas_kcelectra_multilabel_v5"
META_PATH = os.path.join(SAVE_DIR, "meta.json")

DO_TRAIN = False
SEED = 42

EPOCHS = 4
LR = 3e-5

MAX_LEN = 128
TRAIN_BS = 32
EVAL_BS = 32

LOG_EVERY = 200
THR_GRID = np.arange(0.05, 0.96, 0.05)

NUM_WORKERS = 0
PIN_MEMORY = True

# âœ… Scheduler ì„¤ì •
LR_SCHEDULER_TYPE = "cosine"   # "linear" ë¡œ ë°”ê¾¸ë©´ linear warmup+decay
WARMUP_RATIO = 0.1            # 0.05~0.1 ì¶”ì²œ

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)
print("cuda available:", torch.cuda.is_available())
print("RUN:", RUN_NAME)
print("NOTE:", CHANGE_NOTE)
print("CONFIG:", f"epochs={EPOCHS}, lr={LR}, train_bs={TRAIN_BS}, eval_bs={EVAL_BS}, max_len={MAX_LEN}")
print("SCHEDULER:", f"type={LR_SCHEDULER_TYPE}, warmup_ratio={WARMUP_RATIO}")

# ============================================================
# 0-1) ì¬í˜„ì„± ê³ ì •
# ============================================================
def set_seed(seed):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False

set_seed(SEED)

# ============================================================
# 1) ë°ì´í„° ë¡œë“œ + ë¼ë²¨ ì •ì˜
# ============================================================
print("ğŸ“¦ Loading KMHaS dataset...")
ds = load_dataset("jeanlee/kmhas_korean_hate_speech")
print("âœ… example:", ds["train"][0])

LABELS_EN = ["origin", "physical", "politics", "profanity", "age", "gender", "race", "religion"]
LABELS_KO = [
    "ì¶œì‹ /ì´ì£¼ë¯¼ í˜ì˜¤",
    "ì™¸ëª¨ ë¹„í•˜",
    "ì •ì¹˜/ì´ë… í˜ì˜¤",
    "ì¼ë°˜ ìš•ì„¤",
    "ì—°ë ¹ ë¹„í•˜",
    "ì„±ë³„ í˜ì˜¤",
    "ì¸ì¢… í˜ì˜¤",
    "ì¢…êµ í˜ì˜¤",
]
num_labels = len(LABELS_EN)

print("âœ… num_labels:", num_labels)
print("âœ… LABELS_EN:", LABELS_EN)

# ============================================================
# 2) í† í¬ë‚˜ì´ì €/ì½œë ˆì´í„°
# ============================================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

# ============================================================
# 3) Dataset -> multi-hot + tokenize
# ============================================================
def to_multihot(example):
    y = np.zeros(num_labels, dtype=np.float32)
    for idx in example["label"]:
        if idx == 8:
            continue
        if 0 <= idx < 8:
            y[idx] = 1.0
    example["labels"] = y
    return example

def tokenize(batch):
    tok = tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN
    )
    tok["labels"] = batch["labels"]
    return tok

def build_loaders():
    ds2 = ds.map(to_multihot, desc="to_multihot")

    remove_cols = ds2["train"].column_names
    ds_tok = ds2.map(tokenize, batched=True, remove_columns=remove_cols, desc="tokenize")
    ds_tok.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    g = torch.Generator()
    g.manual_seed(SEED)

    train_loader = DataLoader(
        ds_tok["train"],
        batch_size=TRAIN_BS,
        shuffle=True,
        generator=g,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        collate_fn=data_collator
    )
    val_loader = DataLoader(
        ds_tok["validation"],
        batch_size=EVAL_BS,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        collate_fn=data_collator
    )
    return train_loader, val_loader

# ============================================================
# 4) metrics/threshold íŠœë‹
# ============================================================
def collect_probs_and_labels(model, loader):
    model.eval()
    probs_all, labels_all = [], []
    with torch.no_grad():
        for batch in loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            logits = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            ).logits
            probs = torch.sigmoid(logits).cpu().numpy()
            probs_all.append(probs)
            labels_all.append(batch["labels"].cpu().numpy().astype(int))
    return np.vstack(probs_all), np.vstack(labels_all)

def eval_metrics_from_probs_flatten(probs, labels, thr_vec):
    from sklearn.metrics import f1_score, accuracy_score
    preds = (probs >= thr_vec.reshape(1, -1)).astype(int)

    preds_flat = preds.reshape(-1)
    labels_flat = labels.reshape(-1)

    micro_f1 = f1_score(labels_flat, preds_flat, average="micro", zero_division=0)
    macro_f1 = f1_score(labels_flat, preds_flat, average="macro", zero_division=0)
    acc = accuracy_score(labels_flat, preds_flat)

    return float(micro_f1), float(macro_f1), float(acc)

def tune_thresholds_per_label(probs, labels, grid):
    from sklearn.metrics import f1_score
    best_thr = np.full(labels.shape[1], 0.5, dtype=np.float32)

    for i in range(labels.shape[1]):
        y_true = labels[:, i]
        if y_true.sum() == 0:
            best_thr[i] = 0.5
            continue

        best_f1, best_t = -1.0, 0.5
        for t in grid:
            y_pred = (probs[:, i] >= t).astype(int)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            if f1 > best_f1:
                best_f1, best_t = f1, float(t)

        best_thr[i] = best_t

    return best_thr

# ============================================================
# 5) ë©”íƒ€ ì €ì¥/ë¡œë“œ
# ============================================================
def save_meta(thresholds, best_epoch_metrics, tuned_metrics):
    os.makedirs(SAVE_DIR, exist_ok=True)
    meta = {
        "run_name": RUN_NAME,
        "change_note": CHANGE_NOTE,
        "model_name": MODEL_NAME,
        "seed": SEED,

        "num_labels": num_labels,
        "labels_en": LABELS_EN,
        "labels_ko": LABELS_KO,

        "max_len": MAX_LEN,
        "lr": LR,
        "epochs": EPOCHS,
        "train_bs": TRAIN_BS,
        "eval_bs": EVAL_BS,
        "loss": "BCEWithLogitsLoss",
        "weight_decay": 0.01,

        "lr_scheduler_type": LR_SCHEDULER_TYPE,
        "warmup_ratio": WARMUP_RATIO,

        "threshold_grid": [float(x) for x in THR_GRID],
        "thresholds": [float(x) for x in thresholds],

        "best_epoch_metrics_at_0.5_flatten": {
            "val_micro_f1": best_epoch_metrics[0],
            "val_macro_f1": best_epoch_metrics[1],
            "val_acc": best_epoch_metrics[2],
        },
        "tuned_metrics_flatten": {
            "val_micro_f1": tuned_metrics[0],
            "val_macro_f1": tuned_metrics[1],
            "val_acc": tuned_metrics[2],
        }
    }
    with open(META_PATH, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

def load_meta():
    with open(META_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

# ============================================================
# 6) í•™ìŠµ + best ì €ì¥ + threshold íŠœë‹ + meta ì €ì¥
# ============================================================
def train_and_save():
    train_loader, val_loader = build_loaders()

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=num_labels,
        problem_type="multi_label_classification",
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)
    loss_fn = nn.BCEWithLogitsLoss()

    # âœ… LR Scheduler ìƒì„±
    num_training_steps = len(train_loader) * EPOCHS
    num_warmup_steps = int(num_training_steps * WARMUP_RATIO)

    lr_scheduler = get_scheduler(
        name=LR_SCHEDULER_TYPE,
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps,
    )

    print("âœ… Scheduler ready:",
          f"type={LR_SCHEDULER_TYPE}, warmup_steps={num_warmup_steps}, total_steps={num_training_steps}")

    best_micro_f1 = -1.0
    best_state = None
    best_epoch_metrics = (0.0, 0.0, 0.0)

    thr_05 = np.full(num_labels, 0.5, dtype=np.float32)

    print("ğŸš€ Training start...")

    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0.0

        for step, batch in enumerate(train_loader, 1):
            batch = {k: v.to(device) for k, v in batch.items()}

            logits = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            ).logits

            loss = loss_fn(logits, batch["labels"].float())
            optimizer.zero_grad(set_to_none=True)
            loss.backward()

            optimizer.step()
            lr_scheduler.step()

            total_loss += loss.item()

            if step % LOG_EVERY == 0:
                curr_lr = lr_scheduler.get_last_lr()[0]
                print(f"epoch {epoch} step {step}/{len(train_loader)} "
                      f"loss {total_loss/step:.4f} | lr {curr_lr:.8f}")

        val_probs, val_labels = collect_probs_and_labels(model, val_loader)
        val_micro_f1, val_macro_f1, val_acc = eval_metrics_from_probs_flatten(val_probs, val_labels, thr_05)

        train_loss = total_loss / len(train_loader)
        print(
            f"âœ… epoch {epoch} done | "
            f"train_loss {train_loss:.4f} | "
            f"val_micro_f1@0.5 {val_micro_f1:.4f} | "
            f"val_macro_f1@0.5 {val_macro_f1:.4f} | "
            f"val_acc@0.5 {val_acc:.4f}"
        )

        if val_micro_f1 > best_micro_f1:
            best_micro_f1 = val_micro_f1
            best_epoch_metrics = (val_micro_f1, val_macro_f1, val_acc)
            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}

    if best_state is not None:
        model.load_state_dict(best_state)

    os.makedirs(SAVE_DIR, exist_ok=True)
    model.save_pretrained(SAVE_DIR)
    tokenizer.save_pretrained(SAVE_DIR)

    print("ğŸ¯ Tuning per-label thresholds on validation set...")
    val_probs, val_labels = collect_probs_and_labels(model, val_loader)
    thresholds = tune_thresholds_per_label(val_probs, val_labels, THR_GRID)

    tuned_micro_f1, tuned_macro_f1, tuned_acc = eval_metrics_from_probs_flatten(val_probs, val_labels, thresholds)
    print(
        f"âœ… tuned metrics (flatten) | "
        f"val_micro_f1 {tuned_micro_f1:.4f} | "
        f"val_macro_f1 {tuned_macro_f1:.4f} | "
        f"val_acc {tuned_acc:.4f}"
    )

    print("âœ… thresholds (KO):")
    for i in range(num_labels):
        print(i, LABELS_KO[i], "->", round(float(thresholds[i]), 2))

    save_meta(thresholds, best_epoch_metrics, (tuned_micro_f1, tuned_macro_f1, tuned_acc))
    print("ğŸ’¾ Saved finetuned model to:", SAVE_DIR)

    return model, thresholds

# ============================================================
# 7) ë¡œë“œ
# ============================================================
def load_finetuned():
    if not os.path.exists(SAVE_DIR) or not os.path.exists(META_PATH):
        raise RuntimeError("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. DO_TRAIN=Trueë¡œ í•œ ë²ˆ í•™ìŠµ/ì €ì¥ë¶€í„° í•´ì£¼ì„¸ìš”.")

    meta = load_meta()
    print("ğŸ“Œ Loaded meta keys:", list(meta.keys()))
    print("RUN_NAME in meta:", meta.get("run_name"), "| NOTE:", meta.get("change_note"))

    model = AutoModelForSequenceClassification.from_pretrained(SAVE_DIR).to(device)
    model.eval()

    thresholds = np.array(meta.get("thresholds", [0.5] * num_labels), dtype=np.float32)
    labels_ko = meta.get("labels_ko", LABELS_KO)
    return model, thresholds, labels_ko

# ============================================================
# 8) ì‹¤í–‰ ë¶„ê¸°
# ============================================================
if DO_TRAIN:
    v5_model, thresholds = train_and_save()
    labels_ko = LABELS_KO
else:
    v5_model, thresholds, labels_ko = load_finetuned()

# ============================================================
# 9) ë‹¨ë… ì¶”ë¡  ì¶œë ¥ (v5)
# ============================================================
def predict_probs(model, text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model(**inputs).logits[0]
        probs = torch.sigmoid(logits).cpu().numpy()
    return probs

def infer(text):
    t0 = time.time()
    probs = predict_probs(v5_model, text)

    detected = [(labels_ko[i], float(probs[i])) for i in range(num_labels) if probs[i] >= thresholds[i]]
    detected.sort(key=lambda x: x[1], reverse=True)

    ms = (time.time() - t0) * 1000
    return detected, ms

# tests = [
#     "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì‚°ì±…í•˜ëŸ¬ ë‚˜ê°”ë‹¤ ì™”ì–´ìš”.",
#     "ì™€ ì§„ì§œ ì˜¤ëŠ˜ ì¼ ê°œë¹¡ì…Œë‹¤, ë‚˜ ì™„ì „ ë…¹ì´ˆ ëì–´.",
#     "ì—¬ìë“¤ì€ ê°ì •ì ì´ë¼ì„œ ì¤‘ìš”í•œ ì¼ì—ëŠ” ë§¡ê¸°ë©´ ì•ˆ ëœë‹¤.",
#     "ì´ë¯¼ìë“¤ì€ ë‹¤ ì«“ì•„ë‚´ì•¼ ë‚˜ë¼ê°€ ì‚°ë‹¤.",
#     "ëŒ“ê¸€ì— â€œì´ë¯¼ìë“¤ì€ ë‹¤ ì«“ì•„ë‚´ì•¼ í•œë‹¤â€ë¼ëŠ” ê¸€ì´ ì˜¬ë¼ì™€ì„œ ì¶©ê²© ë°›ì•˜ë‹¤.",
# ]

tests = [
    "í•œêµ­ ë‚¨ìë“¤ì€ ë„ˆë¬´ í˜ì˜¤ìŠ¤ëŸ½ë‹¤",
]

print("\n\n==================== (v5) ë‹¨ë… ë©€í‹°ë¼ë²¨ ì¶œë ¥ ====================")
for t in tests:
    detected, ms = infer(t)
    print("\n" + "="*100)
    print("[ì…ë ¥ ë¬¸ì¥]")
    print(t)
    print("-"*100)
    print("ì¶”ë¡  ì‹œê°„(ms):", round(ms, 2))
    if not detected:
        print("ê²€ì¶œ ë¼ë²¨: ì—†ìŒ")
    else:
        print("ê²€ì¶œ ë¼ë²¨:")
        for name, p in detected:
            print("-", name, ":", round(p, 4))
    print("="*100)
